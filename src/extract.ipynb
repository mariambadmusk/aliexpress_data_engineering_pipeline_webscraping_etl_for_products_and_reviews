{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from utils import setup_logging\n",
    "from itertools import cycle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import product category\n",
    "def get_category():\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Aliexpress_api_get_categories initialised\")\n",
    "\n",
    "    try:\n",
    "        url = 'https://ali-express1.p.rapidapi.com/categories'\n",
    "        headers = {\n",
    "            \"X-RapidAPI-Host\": os.getenv(\"RAPIDAPI_HOST\"),\n",
    "            \"X-RapidAPI-Key\": os.getenv(\"RAPIDAPI_KEY\")\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        categories = response.json()\n",
    "\n",
    "        # create a parent directory\n",
    "        parent_dir = \"raw_data\"\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "    \n",
    "         # write to directory\n",
    "        datetimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(parent_dir, f\"aliexpress-categories-{datetimestamp}.json\")\n",
    "\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(categories, f, indent = 3)\n",
    "        logger.info(f\"Aliexpress categories saved to {path}\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching Aliexpress categories: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fecthing Aliexpress categories: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aliexpress_products_scraper:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.aliexpress.com/fn/search-pc/index\"\n",
    "        self.setup_logging()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"Aliexpress_products_scraper initialised\")\n",
    "        self.headers_cycle = cycle(self.get_headers())\n",
    "\n",
    "\n",
    "    def get_headers(self):\n",
    "        total_headers = os.getenv(\"total_headers\")\n",
    "        headers = []\n",
    "        total_headers = int(total_headers)\n",
    "\n",
    "        for i in range(1, total_headers+1):\n",
    "            headers.append(os.getenv(f\"product_header_{i}\"))\n",
    "        \n",
    "        return headers\n",
    "\n",
    "\n",
    "    def rotate_headers(self):\n",
    "        \"\"\"Rotate headers based on the index.\"\"\"\n",
    "        return next(self.headers_cycle)\n",
    "\n",
    "\n",
    "    def get_payload(self, query, page):\n",
    "        payload = {\"pageVersion\":\"7ece9c0cc9cf2052db74f0d1b26b7033\",\n",
    "            \"target\":\"root\",\n",
    "            \"data\":{\n",
    "                \"isFromCategory\":\"y\",\n",
    "                \"categoryUrlParams\":json.dumps({\n",
    "                    \"q\":query,\n",
    "                    \"s\":\"qp_nw\",\n",
    "                    \"sg_search_params\":\"\",\n",
    "                    \"searchBizScene\":\"openSearch\",\n",
    "                    \"recog_lang\":\"en\", \n",
    "                    \"guideModule\":\"category_navigate_vertical\",\n",
    "                }),\n",
    "                    \"page\":page,\n",
    "                    \"g\":\"y\",\n",
    "                    \"SearchText\":query,\n",
    "                    \"origin\":\"y\"\n",
    "                },\n",
    "                    \"eventName\":\"onChange\",\n",
    "                    \"dependency\":[]\n",
    "                }\n",
    "\n",
    "        return payload\n",
    "    \n",
    "    def fetch_category_dim_from_csv(self, file):\n",
    "        category_dim = pd.read_csv(f\"{file}\")\n",
    "        category_dim.to_dict(orient=\"records\")\n",
    "        return category_dim\n",
    "    \n",
    "\n",
    "    def write_products_details_to_file(self, category, subcategory, products_results):\n",
    "        \"\"\"Write product details to a JSON file.\"\"\"\n",
    "\n",
    "        datetimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        path = f\"rawdata/{category}_{subcategory}_{datetimestamp}.json\"\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"r+\") as f:\n",
    "                    existing_data = json.load(f)\n",
    "                    products_results.extend(existing_data)\n",
    "                    f.seek(0)\n",
    "                    json.dump(products_results, f, indent = 3)\n",
    "            else:\n",
    "                with open(path, \"w\") as f:\n",
    "                    json.dump(products_results, f, indent = 3)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error writing to {path}: {e}\")  \n",
    "        \n",
    "\n",
    "    def extract_products_listings(self, query):\n",
    "\n",
    "        \"\"\"Extract the products listings from the website.\"\"\"\n",
    "        category_dim = self.fetch_category_dim_from_csv\n",
    "        category = category_dim[\"category\"]\n",
    "        subcategory = category_dim[\"subcategory\"]\n",
    "\n",
    "        try:\n",
    "            # extract the first page\n",
    "            payload = self.get_payload(query, 1)\n",
    "            header = self.rotate_headers()\n",
    "            \n",
    "            response = requests.post(self.url, headers=header, json=payload)\n",
    "            response.raise_for_status()            \n",
    "\n",
    "            if response.status_code == 200:\n",
    "                page_1_result = response.json()\n",
    "                total_page_number = page_1_result[\"data\"][\"result\"][\"pageInfo\"][\"totalPage\"]\n",
    "\n",
    "                # save the first page to a file\n",
    "                self.write_products_details_to_file(category, subcategory, page_1_result) \n",
    "\n",
    "                # Loop through the rest of the pages\n",
    "                for page in range(2, total_page_number + 1):\n",
    "                    payload = self.get_payload(query, page)\n",
    "                    header = self.rotate_headers()\n",
    "\n",
    "                    response = requests.post(self.url, headers=header, json=payload)\n",
    "                    response.raise_for_status()\n",
    "                    time.sleep(random.randint(1, 7))\n",
    "\n",
    "                    if response.status_code == 200:\n",
    "                        results = response.json()\n",
    "                        self.write_products_details_to_file(category, subcategory, results) \n",
    "                    else:\n",
    "                        self.logger.error(f\"Error fetching product listings{page}: {response.status_code}\")\n",
    "                        continue\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(\n",
    "                f\"RequestException: {e} in {self.url} for query '{payload['data']['SearchText']}' \"\n",
    "                f\"and page 1 for category '{category}' and subcategory '{subcategory}'\"\n",
    "                    )\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in extract_products_listings: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    scraper = Aliexpress_products_scraper()\n",
    "    scraper.extract_products_listings(\"laptop\", \"electronics\", \"computers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aliexpress_reviews_scraper():\n",
    "    def __init__(self):\n",
    "        self.setup_logging()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"Aliexpress_reviews_scraper initialised\")\n",
    "        self.header_cycle = cycle(self.get_headers())\n",
    "\n",
    "    def get_headers(self):\n",
    "        total_headers = os.getenv(\"total_headers\")\n",
    "        headers = []\n",
    "        total_headers = int(total_headers)\n",
    "\n",
    "        for i in range(1, total_headers+1):\n",
    "            headers.append(os.getenv(f\"product_header_{i}\"))\n",
    "        \n",
    "        return headers\n",
    "\n",
    "\n",
    "    def rotate_headers(self):\n",
    "        \"\"\"Rotate headers based on the index.\"\"\"       \n",
    "        return next(self.header_cycle)\n",
    "    \n",
    "\n",
    "    def fetch_product_ids_from_csv(self, file):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            unretrieved_reviews = df.query(\"reviews_retrieved == False\")\n",
    "            if len(unretrieved_reviews) >= 60:\n",
    "                product_ids_to_process = unretrieved_reviews.iloc[:60].to_dict(orient=\"records\")\n",
    "            else:\n",
    "                product_ids_to_process = unretrieved_reviews.to_dict(orient=\"records\")\n",
    "\n",
    "            return product_ids_to_process\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching product IDs from CSV: {e}\")\n",
    "            return None\n",
    "    \n",
    "\n",
    "    def write_reviews_to_file(self, category, subcategory, new_reviews):\n",
    "        \"\"\"Write scraped reviews to a JSON file.\"\"\"\n",
    "        datetimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        path = f\"rawdata/{category}_{subcategory}_{datetimestamp}_reviews.json\"\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"r+\") as f:\n",
    "                    existing_reviews = json.load(f)\n",
    "                    existing_reviews.extend(new_reviews)\n",
    "                    f.seek(0)\n",
    "                    json.dump(existing_reviews, f, indent = 3)\n",
    "            else:\n",
    "                with open(path, \"w\") as f:\n",
    "                    json.dump(new_reviews, f, indent = 3)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error writing to {path}: {e}\")\n",
    "                \n",
    "\n",
    "    def extract_product_reviews(self, reviews_file_csv):\n",
    "\n",
    "        \"\"\"Extract the product reviews from Aliexpress.\"\"\"\n",
    "\n",
    "        productIDs_dict = self.fetch_product_ids_from_csv(reviews_file_csv)  \n",
    "\n",
    "        for value in productIDs_dict:   \n",
    "            productID = value[\"productID\"]\n",
    "            category = value[\"category\"]\n",
    "            subcategory = value[\"subcategory\"]\n",
    "        \n",
    "            try:\n",
    "            # extract the first page\n",
    "                            \n",
    "                url = f\"https://feedback.aliexpress.com/pc/searchEvaluation.do?productId={productID}&lang=en_US&country=UK&page=1&pageSize=10&filter=all&sort=complex_default\"\n",
    "                payload = {}\n",
    "                header = self.rotate_headers()\n",
    "\n",
    "                response = requests.get(url, headers=header, json=payload)\n",
    "                response.raise_for_status()\n",
    "                time.sleep(random.randint(1, 3))\n",
    "                \n",
    "                if response != 200:\n",
    "                    self.logger.error(f\"Error fetching product reviews: {response.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                page_1_result = response.json()\n",
    "                total_page_number = page_1_result[\"totalPage\"]\n",
    "\n",
    "                if not page_1_result.get(\"records\", []):\n",
    "                    self.logger.info(f\"No reviews found for Product ID: {productID}\")\n",
    "                    continue\n",
    "        \n",
    "                # Write the first page to a file\n",
    "                self.write_reviews_to_file(category, subcategory,  page_1_result)\n",
    "\n",
    "                \n",
    "                # Process the rest of the pages\n",
    "                if total_page_number > 1:\n",
    "                    for page in range(2, total_page_number + 1):\n",
    "                        url = f\"https://feedback.aliexpress.com/pc/searchEvaluation.do?productId={productID}&lang=en_US&country=UK&page={page}&pageSize=10&filter=all&sort=complex_default\"\n",
    "                        payload = {}\n",
    "                        header = self.rotate_headers()\n",
    "\n",
    "                        response = requests.get(url, headers=header, json=payload)\n",
    "                        time.sleep(random.randint(1, 7))\n",
    "\n",
    "                        if response.status_code == 200:\n",
    "                            results = response.json()\n",
    "                            self.write_reviews_to_file(category, subcategory, results)\n",
    "                        else:\n",
    "                            self.logger.error(f\"Error fetching product reviews on page{page} for productID {productID}\")\n",
    "                            continue\n",
    "\n",
    "\n",
    "            except requests.exceptions.RequestExceptionException as e:\n",
    "                self.logger.error(f\"Error fetching reviews for product id: {productID}- {e}\")\n",
    "                continue\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    scraper = Aliexpress_reviews_scraper()\n",
    "    scraper.extract_product_reviews(\"1005001772520001\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
